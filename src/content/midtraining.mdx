---
title: "midtraining"
date: 2025-10-30
---

In just under 10 months since Doria Alexander's essay *What's the deal with mid-training?*, a lot has changed. RL had its ups and downs, there were winners and losers, and Claude even got to run a vending machine. But midtraining seemingly did not get the buzz that Doria predicted, at least not yet. Are we just early?

<aside>
See Doria's original essay: [What's the deal with mid-training?](https://www.interconnects.ai/p/whats-the-deal-with-mid-training)
</aside>

One reason might be that midtraining is often overshadowed by its flashier upstream and downstream neighbors; pretraining has the gravitas of the largest buildout in the history of the states, and posttraining gets all the lovely hill-climbing graphs (with an additional air of mystique at the frontier).

> pretraining is an elegant science, done by mathematicians who sit in cold rooms writing optimization theory on blackboards, engineers with total absorb of distributed systems of titanic scale
> posttraining is hair raising cowboy research where people drinking a lot of diet coke yell new hyperparameters to try at each other across the room. it's doing too many tables! the vibes are getting worse, turn down the knob! checkpoint gpt-9-final-v320-restart4 is calling me names! the goose is loose

<aside>
From [@tszzl on X](https://x.com/tszzl/status/1881832891503231161)
</aside>

What about midtraining, roon? Huh?? You out of funny words?

## Defining Midtraining

For now, I'll just borrow a definition from Octothinker, although definitely check out Doria's piece for more context:

>Mid-training is a mid-stage whose computational and data (token) requirements are intermediate between pre-training and post-training. It aims to achieve specific objectives — such as domain and language expansion [1], long-context extension [2], improving data quality [3], leveraging large-scale synthetic data [4], and preparing for post-training, among others — by significantly altering data quality and distribution [5] (and/or modifying model architecture to improve inference efficiency [6]).

<aside>
Definition from [Octothinker's comprehensive overview](https://octothinker.substack.com/p/midtraining-the-missing-piece-of)
</aside>

<aside>
**[1]** Dou et al. (2025). Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs. [arXiv:2502.12982](https://arxiv.org/abs/2502.12982)
</aside>

<aside>
**[2]** Abdin et al. (2024). Phi-3 Technical Report. [arXiv:2404.14219](https://arxiv.org/abs/2404.14219); Abdin et al. (2024). Phi-4 Technical Report. [arXiv:2412.08905](https://arxiv.org/abs/2412.08905)
</aside>

<aside>
**[3]** Hu et al. (2024). MiniCPM. [arXiv:2404.06395](https://arxiv.org/abs/2404.06395); OLMo Team (2025). 2 OLMo 2 Furious. [arXiv:2501.00656](https://arxiv.org/abs/2501.00656)
</aside>

<aside>
**[4]** Yang et al. (2024). Qwen2.5 Technical Report. [arXiv:2412.15115](https://arxiv.org/abs/2412.15115); Yang et al. (2024). Qwen2.5-Math Technical Report. [arXiv:2409.12122](https://arxiv.org/abs/2409.12122); Yang et al. (2025). Qwen3 Technical Report. [arXiv:2505.09388](https://arxiv.org/abs/2505.09388)
</aside>

<aside>
**[5]** Dubey et al. (2024). Llama 3 Technical Report. [arXiv:2407.21783](https://arxiv.org/abs/2407.21783); Wake et al. (2024). Yi Lightning Technical Report. [arXiv:2412.01253](https://arxiv.org/abs/2412.01253)
</aside>

<aside>
**[6]** Bercovich et al. (2024). Puzzle. [arXiv:2411.19146](https://arxiv.org/abs/2411.19146); Bercovich et al. (2025). Llama-nemotron: Efficient reasoning models. [arXiv:2505.00949](https://arxiv.org/abs/2505.00949)
</aside>

This is a solid picture! Some readers can feel free to stop here. But I feel like it's missing something in the light of more recent tech reports. These examples are all primarily focused on single-turn tasks. What does midtraining look like if you're trying to train an agent?

## Midtraining for Agents

The simulators perspective describes GPT as not globally agentic, yet capable of behaving in goal-directed ways, insofar as it acts as a simulator of agentic simulacra. If we run with this, we can imagine trying to raise the fidelity of our simulator by increasing the amount of goal-directed behavior it encounters (a scarce commodity in generic internet text). Since this doesn't occur naturally in desirable rates, labs resort to large-scale synth playgrounds and rejection sampling.

<aside>
The "simulators" framing comes from [janus's essay](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators) on LessWrong.
</aside>

## Data Rephrasing

This also ties in neatly with one of the other lessons from midtraining: you need to rephrase your data.